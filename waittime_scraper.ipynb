{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyfMq0ku6Jkh"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "import re\n",
        "from urllib.request import Request, urlopen\n",
        "import unicodedata\n",
        "import yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Version I"
      ],
      "metadata": {
        "id": "0zkYZ9YO0Ts_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It appears as though I can't use this method to just scrape for something that spans multiple lines. First turn into a string and search for a substring."
      ],
      "metadata": {
        "id": "ui7KiIPiyib2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://www.leedsandyorkpft.nhs.uk/our-services/gender-identity-service/\"\n",
        "request = Request(\n",
        "    \"https://www.leedsandyorkpft.nhs.uk/our-services/gender-identity-service/\",\n",
        "    headers={'User-Agent': 'Mozilla/5.0'})\n",
        "page = urlopen(request).read().decode('utf-8')\n",
        "\n",
        "soup = BeautifulSoup(page, \"html.parser\")\n",
        "\n",
        "#turn into string\n",
        "soup_string = soup.get_text()\n",
        "\n",
        "# removing \"\\xa0\" from text by converting it to space\n",
        "souped = unicodedata.normalize('NFKD', soup_string)\n",
        "\n",
        "# Next search for the important parts"
      ],
      "metadata": {
        "id": "lNQyZL0wytVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The important parts of the text are here:\n",
        "\n",
        "\n",
        "> This information was updated on Friday, 20 September. We will update the referral information on a three-monthly basis. Please note, we are open to referrals.\n",
        "\n",
        ">There are currently 5934 people on our standard waiting list to be seen.\n",
        "\n",
        ">There are currently 169 people on our priority waiting list waiting to be seen.\n",
        "\n",
        ">We are currently booking appointments for people who were referred in approximately June 2019. If you were referred before this date and have not been contacted, please contact the service.\n",
        "\n"
      ],
      "metadata": {
        "id": "5050sJt3z8FY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary for all holding data in\n",
        "data = {}"
      ],
      "metadata": {
        "id": "1NPGfxR_y2Dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_strings = [\"This information was updated on\",\n",
        "                  \"There are currently\",\n",
        "                  \"We are currently booking appointments for people who were\"]\n",
        "days = ['Monday',\n",
        "        'Tuesday',\n",
        "        'Wednesday',\n",
        "        'Thursday',\n",
        "        'Friday',\n",
        "        'Saturday',\n",
        "        'Sunday']\n",
        "\n",
        "months = ['January',\n",
        "          'February',\n",
        "          'March',\n",
        "          'April',\n",
        "          'May',\n",
        "          'June',\n",
        "          'July',\n",
        "          'August',\n",
        "          'September',\n",
        "          'October',\n",
        "          'November',\n",
        "          'December']"
      ],
      "metadata": {
        "id": "kQxl19kf3P2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding update date"
      ],
      "metadata": {
        "id": "zPBzScDyy78R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find where phrase appears in site\n",
        "update_sentence_loc = souped.find(search_strings[0])\n",
        "# Date ends one character before the next full stop, so find location of that\n",
        "next_stop = souped[update_sentence_loc:].find('.')\n",
        "# Pull out the sentence\n",
        "update_sentence = souped[update_sentence_loc:update_sentence_loc + next_stop]\n",
        "\n",
        "# Find where the day is in the sentence\n",
        "for day in days:\n",
        "  if day in update_sentence:\n",
        "    location = update_sentence.find(day)\n",
        "    break\n",
        "# If no day has been found, raise error\n",
        "else:\n",
        "  raise ValueError(\"No day found within range. Check site for changes.\")\n",
        "\n",
        "# Date in format ie 'Friday, 20 September, 2024'\n",
        "# Pull it out from the rest of the sentence\n",
        "update_date_str = update_sentence[location:]\n",
        "\n",
        "# Convert to datetime format\n",
        "update_date_obj = datetime.strptime(update_date_str, '%A, %d %B, %Y')\n",
        "\n",
        "print (f'Date of update: {update_date_str} ({update_date_obj})')"
      ],
      "metadata": {
        "id": "rjvsl2HUoy-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding waiting list sizes (both standard, and priority)"
      ],
      "metadata": {
        "id": "gIgxTOUhy_sE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find each instance of the beginning of the waiting list sentences\n",
        "matches = [match.start() for match in re.finditer(search_strings[1], souped)]\n",
        "\n",
        "# List for storage of waiting list sizes\n",
        "waiting_list_sizes = []\n",
        "\n",
        "# Find the standard and priority waiting list sizes\n",
        "for match_index in matches:\n",
        "  size = re.search(r'\\d+', souped[match_index:match_index+100])\n",
        "  # If value found, add to waiting list sizes list\n",
        "  if size:\n",
        "    waiting_list_sizes.append(size.group())\n",
        "  else:\n",
        "    raise ValueError(\"Error: Waiting list number not found, check site for changes\")\n",
        "print(waiting_list_sizes)\n",
        "\n",
        "standard_waiting_list_size = waiting_list_sizes[0]\n",
        "priority_waiting_list_size = waiting_list_sizes[1]\n",
        "\n",
        "print(f'Standard waiting list size: {standard_waiting_list_size}, priority list size: {priority_waiting_list_size}')"
      ],
      "metadata": {
        "id": "YD0XT0cA4Kl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, find when they're currently seeing people from"
      ],
      "metadata": {
        "id": "kIAl2dY2HUCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find phrase for final part\n",
        "reftime_sentence_loc = souped.find(search_strings[2])\n",
        "# Find next period in sentence\n",
        "next_stop = souped[reftime_sentence_loc:].find('.')\n",
        "# Pull out sentence\n",
        "reftime_sentence = souped[reftime_sentence_loc:reftime_sentence_loc+next_stop]\n",
        "\n",
        "for month in months:\n",
        "  if month in reftime_sentence:\n",
        "    location = reftime_sentence.find(month)\n",
        "    break\n",
        "# Raise exception if above break is never executed\n",
        "else:\n",
        " raise ValueError(\"No month found in sentence! Check site for changes.\")\n",
        "\n",
        "# Pull out and store date string\n",
        "ref_date_str = reftime_sentence[location:]\n",
        "\n",
        "# Convert date string to obj\n",
        "ref_date_obj = datetime.strptime(ref_date_str, '%B %Y')\n",
        "\n",
        "print (f'Date of referral: {ref_date_str} ({ref_date_obj})')"
      ],
      "metadata": {
        "id": "j14cDpScHZNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now add the data into a dictionary, which can then be put into a list with the other data."
      ],
      "metadata": {
        "id": "W9gGVY8j4jhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define main list, which will hold all dictionaries\n",
        "all_data = []\n",
        "\n",
        " # Add results of previous run into keyed dictionary\n",
        "obtained_data = {'Date': update_date_obj,\n",
        "                 'Date (str)': update_date_str,\n",
        "                 'Standard waiting list': standard_waiting_list_size,\n",
        "                 'Priority waiting list': priority_waiting_list_size,\n",
        "                 'Referral date of appts': ref_date_obj,\n",
        "                 'Referral date of appts (str)': ref_date_str}\n",
        "\n",
        "# Check to see if data already has been collected and added\n",
        "for entry in all_data:\n",
        "  if obtained_data[\"Date\"] == entry[\"Date\"]:\n",
        "    print(\"This update has already been recorded, discarding results.\")\n",
        "    break\n",
        "else:\n",
        "  # Add to list of all data\n",
        "  print(\"Appending new data!\")\n",
        "  all_data.append(obtained_data)\n",
        "\n",
        "# For printing out in a nice way for reading it if you're a human.\n",
        "yaml.Dumper.ignore_aliases = lambda *args : True # don't use aliases\n",
        "print(yaml.dump(all_data, allow_unicode=True, default_flow_style=False))"
      ],
      "metadata": {
        "id": "lkbMFGBTHkLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obtained_data[\"Date\"]"
      ],
      "metadata": {
        "id": "3oTH5FLZHzez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next steps:\n",
        "- Fix the fact that it will find whichever day it found last (maybe limit the area it searches for days or months, as if it finds January first but then September comes up later, that will be the result - fix this by only searching between the sentence start found location index and that of the next full stop, or\n",
        "- Clean up top of page\n",
        "- Add data to dictionary (can modify example I created below)\n",
        "- Find out how to use wayback machine API and begin to code for this (may not be back online yet, but try API anyway)\n",
        "- Add other data to CSV and put it in the correct format so it can be analysed and trends extrapolated."
      ],
      "metadata": {
        "id": "R3uV8fc-S1XW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make data a list of dictionaries? Can be converted to something else later."
      ],
      "metadata": {
        "id": "6lb7uxNc5aEU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wayback API"
      ],
      "metadata": {
        "id": "QrCS56Ijke-H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Having a look at using this https://pypi.org/project/waybackpy/"
      ],
      "metadata": {
        "id": "iyU_KvqAl9kx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "!pip install waybackpy"
      ],
      "metadata": {
        "id": "8QksWzW0lR60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from waybackpy import WaybackMachineCDXServerAPI\n",
        "\n",
        "# Dates to search between:\n",
        "datefrom = 2017\n",
        "dateto   = 2018\n",
        "\n",
        "url = \"https://www.leedsandyorkpft.nhs.uk/our-services/gender-identity-service/\"\n",
        "\n",
        "user_agent = \"Mozilla/5.0 (Windows NT 5.1; rv:40.0) Gecko/20100101 Firefox/40.0\"\n",
        "\n",
        "cdx = WaybackMachineCDXServerAPI(url, user_agent, start_timestamp=datefrom, end_timestamp=dateto)\n",
        "\n",
        "for item in cdx.snapshots():\n",
        "  print(item.archive.url)\n",
        "\n"
      ],
      "metadata": {
        "id": "Vmzq_f53ksHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively try https://github.com/edgi-govdata-archiving/wayback"
      ],
      "metadata": {
        "id": "KEDullWx70ex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install wayback"
      ],
      "metadata": {
        "id": "Ol0g67fB78ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wayback\n",
        "client = wayback.WaybackClient()"
      ],
      "metadata": {
        "id": "pXuNVrBu8Yol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for record in client.search('http://nasa.gov', from_date=(2021, 1, 1), to_date=(2021, 3, 1)): # url can be replaced with url variable later\n",
        "  memento = client.get_memento(record)"
      ],
      "metadata": {
        "id": "FARwB6pc8Buq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notes\n",
        "When it's checking more than one version it will need to be able to see if the upload date matches any of the previous ones stored and disregard that update.\n",
        "\n",
        "Would be nice to be able to see the difference between the date of update and the date they're seeing people from on a graph over time."
      ],
      "metadata": {
        "id": "_YM_mP9Zu_Ei"
      }
    }
  ]
}